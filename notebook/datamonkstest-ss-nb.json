{
	"name": "datamonkstest-ss-nb",
	"properties": {
		"folder": {
			"name": "datamonkstestss"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "ETLChapterSPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "893e6025-f548-4fc6-80b4-1c1c6f144d01"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/a802af10-3082-4ea6-b3db-542043a41e96/resourceGroups/rg-dpi30-test-eus-001/providers/Microsoft.Synapse/workspaces/dpi30-test-eus-syn-01/bigDataPools/ETLChapterSPool",
				"name": "ETLChapterSPool",
				"type": "Spark",
				"endpoint": "https://dpi30-test-eus-syn-01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ETLChapterSPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"storageAccountName = \"dpi30testeusdlsarz01\"\n",
					"containerName = \"rz-container\"\n",
					"path = \"Pricing/datamonks-test-ss\"\n",
					"sasToken = \"sp=r&st=2022-04-18T06:53:47Z&se=2022-04-18T14:53:47Z&spr=https&sv=2020-08-04&sr=b&sig=6bV0K169gREKZW9VxDWYA9KlKSfcIm7gbR7CmzmrgVQ%3D\"\n",
					"\n",
					"config = {\n",
					"    \"fs.azure.sas.\" + containerName+ \".\" + storageAccountName + \".blob.core.windows.net\" : sasToken\n",
					"}\n",
					""
				],
				"execution_count": 51
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_dbutils(spark):\n",
					"        try:\n",
					"            from pyspark.dbutils import DBUtils\n",
					"            dbutils = DBUtils(spark)\n",
					"        except ImportError:\n",
					"            import IPython\n",
					"            dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n",
					"        return dbutils\n",
					"\n",
					"dbutils = get_dbutils(spark)"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"dbutils.fs.mount(\n",
					"  source = f\"wasbs://{containerName}@{storageAccountName}.blob.core.windows.net/{path}\",\n",
					"  mountPoint = \"/mnt/myfile\",\n",
					"  extraConfigs = config)\n",
					"\n",
					"# val fullpath = f\"wasbs://{containerName}@{storageAccountName}.blob.core.windows.net/{}\"\n",
					"# spark.conf.set(f\"fs.azure.sas.$container_name.$acc_name.blob.core.windows.net\",sasToken)\n",
					"# // load the sample data as a Spark DataFrame\n",
					"# val df = spark.read.parquet(fullpath+\"userdata2.parquet\") \n",
					"# df.show(5, truncate = false)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from notebookutils import mssparkutils  \n",
					"\n",
					"\n",
					"\n",
					"# mssparkutils.fs.mount( \n",
					"#     f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/{path}\",\n",
					"#     \"/test\", \n",
					"#     {\"linkedService\":\"AzureDataLakeStorageDatamonksTest\"} \n",
					"# ) \n",
					"\n",
					"print(\"starting unmount\")\n",
					"\n",
					"mssparkutils.fs.unmount(\"/test\")\n",
					"\n",
					"print(\"starting mount\")\n",
					"\n",
					"mssparkutils.fs.mount(  \n",
					"    f\"abfss://{containerName}@{storageAccountName}.dfs.core.windows.net/{path}\",  \n",
					"    \"/test\",  \n",
					"    {\"accountKey\":\"2GAsDUhfVC6iKvE3mSWylpAQx6LuqFrstjS/zaxobkM7jj4IB8+f1+Sh2x6wQv5nSQtSiUql8vSf+AStui8BGg==\"}\n",
					") "
				],
				"execution_count": 52
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"jobId = mssparkutils.env.getJobId() \n",
					"print(jobId)\n",
					"mssparkutils.fs.ls(f\"synfs:/{jobId}/test\")\n",
					"\n",
					"f = open(f\"/synfs/{jobId}/test/myFile.txt\", \"a\") \n",
					"\n",
					"f.write(\"Hello world. test2\") \n",
					"f.close() "
				],
				"execution_count": 54
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import os\n",
					" \n",
					"# Get the list of all files and directories\n",
					"path = \"/\"\n",
					"dir_list = os.listdir(path)\n",
					" \n",
					"print(\"Files and directories in '\", path, \"' :\")\n",
					" \n",
					"# prints all files\n",
					"print(dir_list)"
				],
				"execution_count": 14
			}
		]
	}
}